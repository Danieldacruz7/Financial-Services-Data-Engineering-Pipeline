{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Raw_Zone.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import datetime\n",
        "from os import path"
      ],
      "metadata": {
        "id": "-IGk_OoYA5ua"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extraction_and_loading(customer_file, transaction_file):\n",
        "  print(\"Reading customer file...\")\n",
        "  customer_data = spark.read.csv(customer_file,\n",
        "                             sep=',',\n",
        "                             header=True,\n",
        "                             inferSchema=True,\n",
        "                             nullValue='NA')\n",
        "    \n",
        "  print(\"Cleaning customer file data...\")\n",
        "  customer_data = customer_data.dropDuplicates().sort('customerid')\n",
        "\n",
        "  print(\"\\nFirst 5 rows of Customer data:\")\n",
        "  customer_data.show(5)\n",
        "    \n",
        "  print(\"Printing table schema:\")\n",
        "  customer_data.printSchema()\n",
        "    \n",
        "  customer_data.write.format('parquet').save('/content/drive/MyDrive/data_engineer_assessment/raw_zone/raw_parquet_files/customer_data_{}_{}_{}.parquet'.format(date.day, date.month, date.year))\n",
        "  print(\"\\nCustomer file data successfully written to parquet.\")\n",
        "  print(\"---------------------------------------------\")\n",
        "\n",
        "  print(\"\\nReading transaction file data...\")\n",
        "  transaction_data = spark.read.csv(transaction_file,\n",
        "                         sep=',',\n",
        "                         header=True,\n",
        "                         inferSchema=True,\n",
        "                         nullValue='NA')\n",
        "    \n",
        "  print(\"Printing table schema:\")\n",
        "  transaction_data.printSchema()\n",
        "\n",
        "  print(\"\\nCleaning transaction file data...\")\n",
        "  transaction_data = transaction_data.dropDuplicates().sort('transaction_date')\n",
        "  transaction_data = transaction_data.withColumnRenamed(\"_c0\", \"transaction_id\")\n",
        "\n",
        "  print(\"\\nFirst 5 rows of transaction file data:\")\n",
        "  transaction_data.show(5)\n",
        "\n",
        "  transaction_data.write.format('parquet').save('/content/drive/MyDrive/data_engineer_assessment/raw_zone/raw_parquet_files/transaction_data_{}_{}_{}.parquet'.format(date.day, date.month, date.year))\n",
        "  print(\"\\nTransaction file data successfully written to parquet.\")"
      ],
      "metadata": {
        "id": "VZULJ45lzZpN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory = \"/content/drive/MyDrive/data_engineer_assessment/source_data\"\n",
        "\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "\n",
        "  try:\n",
        "    result = bool(dt.strptime(filename, '%d-%m-%Y')) # Checks whether name of file contains date within source data\n",
        "  \n",
        "    if result == True:\n",
        "      match = re.search(r'\\d{2}-\\d{2}-\\d{4}', filename)\n",
        "      date = datetime.datetime.strptime(match.group(), '%d-%m-%Y').date() # Retrieves date from file\n",
        "\n",
        "      customer_file_name = \"/content/drive/MyDrive/data_engineer_assessment/refined_zone/customer_data_{}_{}_{}.parquet\".format(date.day, date.month, date.year)\n",
        "      customer_file = \"/content/drive/MyDrive/data_engineer_assessment/landing_zone/{}-0{}-{}/customers_{}-0{}-{}.csv\".format(date.day, date.month, date.year, date.day, date.month, date.year)\n",
        "      transaction_file_name = \"/content/drive/MyDrive/data_engineer_assessment/refined_zone/transaction_data_{}_{}_{}.parquet\".format(date.day, date.month, date.year)\n",
        "      transaction_file = \"/content/drive/MyDrive/data_engineer_assessment/landing_zone/{}-0{}-{}/transactions_{}-0{}-{} 00_00_00.csv\".format(date.day, date.month, date.year, date.year, date.month, date.day)\n",
        "\n",
        "      refined_directory = \"/content/drive/MyDrive/data_engineer_assessment/refined_zone/\"\n",
        "\n",
        "      if path.exists(customer_file_name) == False and path.exists(transaction_file_name) == False: # Checks whether filename is within source data folder\n",
        "        extraction_and_loading(customer_file, transaction_file)\n",
        "\n",
        "    if result == False:\n",
        "      continue\n",
        "\n",
        "  except:\n",
        "    continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcLsjMg-VGuj",
        "outputId": "667cc93c-0357-4b3b-f540-48371af6bd67"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading customer file: /content/drive/MyDrive/data_engineer_assessment/landing_zone/15-01-2022/customers_15-01-2022.csv...\n",
            "Cleaning customer file data...\n",
            "\n",
            "First 5 rows of Customer data:\n",
            "+----------+-------------------+-----------------+------------+-----------------+---------------+\n",
            "|customerid|          birthdate|bank_account_type|   bank_name|employment_status|education_level|\n",
            "+----------+-------------------+-----------------+------------+-----------------+---------------+\n",
            "|      1000|1973-10-10 00:00:00|          Savings|Capitec Bank|             null|           null|\n",
            "|      1001|1986-01-21 00:00:00|          Savings|Capitec Bank|        Permanent|           null|\n",
            "|      1002|1987-04-01 00:00:00|          Savings|   Tyme Bank|             null|           null|\n",
            "|      1003|1991-07-19 00:00:00|          Savings|Capitec Bank|        Permanent|           null|\n",
            "|      1004|1982-11-22 00:00:00|          Savings|Capitec Bank|        Permanent|           null|\n",
            "+----------+-------------------+-----------------+------------+-----------------+---------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Printing table schema:\n",
            "root\n",
            " |-- customerid: integer (nullable = true)\n",
            " |-- birthdate: string (nullable = true)\n",
            " |-- bank_account_type: string (nullable = true)\n",
            " |-- bank_name: string (nullable = true)\n",
            " |-- employment_status: string (nullable = true)\n",
            " |-- education_level: string (nullable = true)\n",
            "\n",
            "Reading customer file: /content/drive/MyDrive/data_engineer_assessment/landing_zone/15-03-2022/customers_15-03-2022.csv...\n",
            "Cleaning customer file data...\n",
            "\n",
            "First 5 rows of Customer data:\n",
            "+----------+-------------------+-----------------+-------------+-----------------+---------------+\n",
            "|customerid|          birthdate|bank_account_type|    bank_name|employment_status|education_level|\n",
            "+----------+-------------------+-----------------+-------------+-----------------+---------------+\n",
            "|      1005|1978-12-11 00:00:00|          Savings| Capitec Bank|        Permanent|           null|\n",
            "|      1006|1990-07-21 00:00:00|          Savings|  Access Bank|        Permanent|           null|\n",
            "|      1009|1965-02-23 00:00:00|          Savings|Standard Bank|        Permanent|           null|\n",
            "|      1012|1994-06-03 00:00:00|            Other|    ABSA Bank|        Permanent|      Secondary|\n",
            "|      1018|1983-05-08 00:00:00|          Savings|          FNB|        Permanent|       Graduate|\n",
            "+----------+-------------------+-----------------+-------------+-----------------+---------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Printing table schema:\n",
            "root\n",
            " |-- customerid: integer (nullable = true)\n",
            " |-- birthdate: string (nullable = true)\n",
            " |-- bank_account_type: string (nullable = true)\n",
            " |-- bank_name: string (nullable = true)\n",
            " |-- employment_status: string (nullable = true)\n",
            " |-- education_level: string (nullable = true)\n",
            "\n",
            "Reading customer file: /content/drive/MyDrive/data_engineer_assessment/landing_zone/15-02-2022/customers_15-02-2022.csv...\n",
            "Cleaning customer file data...\n",
            "\n",
            "First 5 rows of Customer data:\n",
            "+----------+-------------------+-----------------+------------+-----------------+---------------+\n",
            "|customerid|          birthdate|bank_account_type|   bank_name|employment_status|education_level|\n",
            "+----------+-------------------+-----------------+------------+-----------------+---------------+\n",
            "|      1003|1991-07-19 00:00:00|          Savings|Capitec Bank|        Permanent|           null|\n",
            "|      1004|1982-11-22 00:00:00|          Savings|         FNB|        Permanent|  Post-Graduate|\n",
            "|      1005|1978-12-11 00:00:00|          Savings|Capitec Bank|        Permanent|           null|\n",
            "|      1006|1990-07-21 00:00:00|          Savings| Access Bank|        Permanent|           null|\n",
            "|      1007|1986-09-09 00:00:00|          Savings|Capitec Bank|        Permanent|           null|\n",
            "+----------+-------------------+-----------------+------------+-----------------+---------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Printing table schema:\n",
            "root\n",
            " |-- customerid: integer (nullable = true)\n",
            " |-- birthdate: string (nullable = true)\n",
            " |-- bank_account_type: string (nullable = true)\n",
            " |-- bank_name: string (nullable = true)\n",
            " |-- employment_status: string (nullable = true)\n",
            " |-- education_level: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "date = datetime.today() # Retrieves the date today to create history of when pipeline started.\n",
        "\n",
        "transaction_list = []\n",
        "\n",
        "directory = \"/content/drive/MyDrive/data_engineer_assessment/raw_zone/raw_parquet_files\"\n",
        "for i in os.listdir(directory):\n",
        "  transaction_list.append(i)\n",
        "  transactions_filtered = fnmatch.filter(transaction_list, 'transaction*') # Find all files with transaction data\n",
        "  customer_filtered = fnmatch.filter(transaction_list, 'customer*') # Finds all files with customer data"
      ],
      "metadata": {
        "id": "bXyqQkTzujWi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transaction_data = spark.read.parquet(\"/content/drive/MyDrive/data_engineer_assessment/raw_zone/raw_parquet_files/{}\".format(transactions_filtered[0])) # Reads first parquet file in transactions list \n",
        "customer_data = spark.read.parquet(\"/content/drive/MyDrive/data_engineer_assessment/raw_zone/raw_parquet_files/{}\".format(customer_filtered[0])) # Reads first parquet file in customers list\n",
        "\n",
        "for i in range(1, len(transactions_filtered)): # Loops through all available transcation files \n",
        "  transaction_data.write.mode('append').parquet(transactions_filtered[i])\n",
        "\n",
        "for i in range(1, len(customer_filtered)): # Loops through all available customer files\n",
        "  customer_data.write.mode('append').parquet(customer_filtered[i])\n",
        "\n",
        "# Creates one main parquet file with all other parquet files appended. \n",
        "transaction_data.write.format('parquet').mode('overwrite').partitionBy(\"transaction_id\").save('/content/drive/MyDrive/data_engineer_assessment/refined_zone/Refined_Parquet_Files/transaction_data_{}_{}_{}.parquet'.format(date.day, date.month, date.year))\n",
        "\n",
        "# Creates one main parquet file with all other parquet files appended. \n",
        "customer_data.write.format('parquet').mode('overwrite').partitionBy(\"customerid\").save('/content/drive/MyDrive/data_engineer_assessment/refined_zone/Refined_Parquet_Files/customer_data_{}_{}_{}.parquet'.format(date.day, date.month, date.year))"
      ],
      "metadata": {
        "id": "zyuwFiHiunP1"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}