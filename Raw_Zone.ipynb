{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Raw_Zone.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import datetime\n",
        "from os import path"
      ],
      "metadata": {
        "id": "-IGk_OoYA5ua"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_file = \"/content/drive/MyDrive/data_engineer_assessment/source_data/15-01-2022/customers_15-01-2022.csv\"\n",
        "transaction_file = \"/content/drive/MyDrive/data_engineer_assessment/source_data/15-01-2022/transactions_2022-01-15 00_00_00.csv\"\n",
        "\n",
        "def extraction_and_loading(customer_file, transaction_file):\n",
        "  \"\"\"Data is to be extracted from CSV file and saved as Parquet file \n",
        "     where duplicates have been removed and column renamed.\"\"\"\n",
        "    \n",
        "    print(\"Reading customer file data...\")\n",
        "    customer_data = spark.read.csv(customer_file,\n",
        "                             sep=',',\n",
        "                             header=True,\n",
        "                             inferSchema=True,\n",
        "                             nullValue='NA')\n",
        "    \n",
        "    print(\"Cleaning customer file data...\")\n",
        "    customer_data = customer_data.dropDuplicates().sort('customerid')\n",
        "\n",
        "    print(\"\\nFirst 5 rows of Customer data:\")\n",
        "    customer_data.show(5)\n",
        "    \n",
        "    print(\"Printing table schema:\")\n",
        "    customer_data.printSchema()\n",
        "    \n",
        "    customer_data.write.format('parquet').save('/content/drive/MyDrive/data_engineer_assessment/raw_zone/raw_parquet_files/customer_data_{}_{}_{}.parquet'.format(date.day, date.month, date.year))\n",
        "    print(\"\\nCustomer file data successfully written to parquet.\")\n",
        "    print(\"---------------------------------------------\")\n",
        "\n",
        "    print(\"\\nReading transaction file data...\")\n",
        "    transaction_data = spark.read.csv(transaction_file,\n",
        "                         sep=',',\n",
        "                         header=True,\n",
        "                         inferSchema=True,\n",
        "                         nullValue='NA')\n",
        "    \n",
        "    print(\"Printing table schema:\")\n",
        "    transaction_data.printSchema()\n",
        "\n",
        "    print(\"\\nCleaning transaction file data...\")\n",
        "    transaction_data = transaction_data.dropDuplicates().sort('transaction_date')\n",
        "    transaction_data = transaction_data.withColumnRenamed(\"_c0\", \"transaction_id\")\n",
        "\n",
        "    print(\"\\nFirst 5 rows of transaction file data:\")\n",
        "    transaction_data.show(5)\n",
        "\n",
        "    transaction_data.write.format('parquet').save('/content/drive/MyDrive/data_engineer_assessment/raw_zone/raw_parquet_files/transaction_data_{}_{}_{}.parquet'.format(date.day, date.month, date.year))\n",
        "    print(\"\\nTransaction file data successfully written to parquet.\")"
      ],
      "metadata": {
        "id": "VZULJ45lzZpN"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory = \"/content/drive/MyDrive/data_engineer_assessment/source_data\"\n",
        "\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "\n",
        "  try:\n",
        "    result = bool(dt.strptime(filename, '%d-%m-%Y')) # Checks whether name of file contains date within source data\n",
        "  \n",
        "    if result == True:\n",
        "      match = re.search(r'\\d{2}-\\d{2}-\\d{4}', filename)\n",
        "      date = datetime.datetime.strptime(match.group(), '%d-%m-%Y').date() # Retrieves date from file\n",
        "\n",
        "      customer_file_name = \"/content/drive/MyDrive/data_engineer_assessment/refined_zone/customer_data_{}_{}_{}.parquet\".format(date.day, date.month, date.year)\n",
        "      customer_file = \"/content/drive/MyDrive/data_engineer_assessment/landing_zone/{}-0{}-{}/customers_{}-0{}-{}.csv\".format(date.day, date.month, date.year, date.day, date.month, date.year)\n",
        "      transaction_file_name = \"/content/drive/MyDrive/data_engineer_assessment/refined_zone/transaction_data_{}_{}_{}.parquet\".format(date.day, date.month, date.year)\n",
        "      transaction_file = \"/content/drive/MyDrive/data_engineer_assessment/landing_zone/{}-0{}-{}/transactions_{}-0{}-{} 00_00_00.csv\".format(date.day, date.month, date.year, date.year, date.month, date.day)\n",
        "\n",
        "      refined_directory = \"/content/drive/MyDrive/data_engineer_assessment/refined_zone/\"\n",
        "\n",
        "      if path.exists(customer_file_name) == False: # Checks whether filename is within source data folder\n",
        "        extraction_and_loading(customer_file, transaction_file)\n",
        "\n",
        "    if result == False:\n",
        "      continue\n",
        "\n",
        "  except ValueError:\n",
        "    continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcLsjMg-VGuj",
        "outputId": "08df3c77-88a6-4a01-b507-db052355ec98"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading customer file data...\n",
            "Cleaning customer file data...\n",
            "\n",
            "First 5 rows of Customer data:\n",
            "+----------+-------------------+-----------------+------------+-----------------+---------------+\n",
            "|customerid|          birthdate|bank_account_type|   bank_name|employment_status|education_level|\n",
            "+----------+-------------------+-----------------+------------+-----------------+---------------+\n",
            "|      1000|1973-10-10 00:00:00|          Savings|Capitec Bank|             null|           null|\n",
            "|      1001|1986-01-21 00:00:00|          Savings|Capitec Bank|        Permanent|           null|\n",
            "|      1002|1987-04-01 00:00:00|          Savings|   Tyme Bank|             null|           null|\n",
            "|      1003|1991-07-19 00:00:00|          Savings|Capitec Bank|        Permanent|           null|\n",
            "|      1004|1982-11-22 00:00:00|          Savings|Capitec Bank|        Permanent|           null|\n",
            "+----------+-------------------+-----------------+------------+-----------------+---------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Printing table schema:\n",
            "root\n",
            " |-- customerid: integer (nullable = true)\n",
            " |-- birthdate: string (nullable = true)\n",
            " |-- bank_account_type: string (nullable = true)\n",
            " |-- bank_name: string (nullable = true)\n",
            " |-- employment_status: string (nullable = true)\n",
            " |-- education_level: string (nullable = true)\n",
            "\n",
            "\n",
            "Customer file data successfully written to parquet.\n",
            "---------------------------------------------\n",
            "\n",
            "Reading transaction file data...\n",
            "Printing table schema:\n",
            "root\n",
            " |-- _c0: integer (nullable = true)\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- transaction_amount: double (nullable = true)\n",
            " |-- transaction_date: string (nullable = true)\n",
            "\n",
            "\n",
            "Cleaning transaction file data...\n",
            "\n",
            "First 5 rows of transaction file data:\n",
            "+--------------+-----------+------------------+--------------------+\n",
            "|transaction_id|customer_id|transaction_amount|    transaction_date|\n",
            "+--------------+-----------+------------------+--------------------+\n",
            "|            87|       1085|             536.0|01-01-2022 02:24:...|\n",
            "|            88|       1034|             420.0|01-01-2022 07:24:...|\n",
            "|            89|       1089|            -253.0|01-01-2022 12:24:...|\n",
            "|            90|       1087|               7.0|01-01-2022 17:23:...|\n",
            "|            91|       1082|             -38.0|01-01-2022 22:23:...|\n",
            "+--------------+-----------+------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Transaction file data successfully written to parquet.\n",
            "Reading customer file data...\n",
            "Cleaning customer file data...\n",
            "\n",
            "First 5 rows of Customer data:\n",
            "+----------+-------------------+-----------------+-------------+-----------------+---------------+\n",
            "|customerid|          birthdate|bank_account_type|    bank_name|employment_status|education_level|\n",
            "+----------+-------------------+-----------------+-------------+-----------------+---------------+\n",
            "|      1005|1978-12-11 00:00:00|          Savings| Capitec Bank|        Permanent|           null|\n",
            "|      1006|1990-07-21 00:00:00|          Savings|  Access Bank|        Permanent|           null|\n",
            "|      1009|1965-02-23 00:00:00|          Savings|Standard Bank|        Permanent|           null|\n",
            "|      1012|1994-06-03 00:00:00|            Other|    ABSA Bank|        Permanent|      Secondary|\n",
            "|      1018|1983-05-08 00:00:00|          Savings|          FNB|        Permanent|       Graduate|\n",
            "+----------+-------------------+-----------------+-------------+-----------------+---------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Printing table schema:\n",
            "root\n",
            " |-- customerid: integer (nullable = true)\n",
            " |-- birthdate: string (nullable = true)\n",
            " |-- bank_account_type: string (nullable = true)\n",
            " |-- bank_name: string (nullable = true)\n",
            " |-- employment_status: string (nullable = true)\n",
            " |-- education_level: string (nullable = true)\n",
            "\n",
            "\n",
            "Customer file data successfully written to parquet.\n",
            "---------------------------------------------\n",
            "\n",
            "Reading transaction file data...\n",
            "Printing table schema:\n",
            "root\n",
            " |-- _c0: integer (nullable = true)\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- transaction_amount: double (nullable = true)\n",
            " |-- transaction_date: string (nullable = true)\n",
            "\n",
            "\n",
            "Cleaning transaction file data...\n",
            "\n",
            "First 5 rows of transaction file data:\n",
            "+--------------+-----------+------------------+--------------------+\n",
            "|transaction_id|customer_id|transaction_amount|    transaction_date|\n",
            "+--------------+-----------+------------------+--------------------+\n",
            "|            73|       1076|            -111.0|01-03-2022 04:30:...|\n",
            "|            74|       1040|             198.0|01-03-2022 09:30:...|\n",
            "|            75|       1096|            -403.0|01-03-2022 14:29:...|\n",
            "|            76|       1056|             203.0|01-03-2022 19:29:...|\n",
            "|            77|       1078|             -42.0|02-03-2022 00:29:...|\n",
            "+--------------+-----------+------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Transaction file data successfully written to parquet.\n",
            "Reading customer file data...\n",
            "Cleaning customer file data...\n",
            "\n",
            "First 5 rows of Customer data:\n",
            "+----------+-------------------+-----------------+------------+-----------------+---------------+\n",
            "|customerid|          birthdate|bank_account_type|   bank_name|employment_status|education_level|\n",
            "+----------+-------------------+-----------------+------------+-----------------+---------------+\n",
            "|      1003|1991-07-19 00:00:00|          Savings|Capitec Bank|        Permanent|           null|\n",
            "|      1004|1982-11-22 00:00:00|          Savings|         FNB|        Permanent|  Post-Graduate|\n",
            "|      1005|1978-12-11 00:00:00|          Savings|Capitec Bank|        Permanent|           null|\n",
            "|      1006|1990-07-21 00:00:00|          Savings| Access Bank|        Permanent|           null|\n",
            "|      1007|1986-09-09 00:00:00|          Savings|Capitec Bank|        Permanent|           null|\n",
            "+----------+-------------------+-----------------+------------+-----------------+---------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Printing table schema:\n",
            "root\n",
            " |-- customerid: integer (nullable = true)\n",
            " |-- birthdate: string (nullable = true)\n",
            " |-- bank_account_type: string (nullable = true)\n",
            " |-- bank_name: string (nullable = true)\n",
            " |-- employment_status: string (nullable = true)\n",
            " |-- education_level: string (nullable = true)\n",
            "\n",
            "\n",
            "Customer file data successfully written to parquet.\n",
            "---------------------------------------------\n",
            "\n",
            "Reading transaction file data...\n",
            "Printing table schema:\n",
            "root\n",
            " |-- _c0: integer (nullable = true)\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- transaction_amount: double (nullable = true)\n",
            " |-- transaction_date: string (nullable = true)\n",
            "\n",
            "\n",
            "Cleaning transaction file data...\n",
            "\n",
            "First 5 rows of transaction file data:\n",
            "+--------------+-----------+------------------+--------------------+\n",
            "|transaction_id|customer_id|transaction_amount|    transaction_date|\n",
            "+--------------+-----------+------------------+--------------------+\n",
            "|            87|       1040|             660.0|01-02-2022 02:24:...|\n",
            "|            88|       1030|              43.0|01-02-2022 07:24:...|\n",
            "|            89|       1033|            -422.0|01-02-2022 12:24:...|\n",
            "|            90|       1050|            -273.0|01-02-2022 17:23:...|\n",
            "|            91|       1016|             678.0|01-02-2022 22:23:...|\n",
            "+--------------+-----------+------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Transaction file data successfully written to parquet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "date = datetime.today() # Retrieves the date today to create history of when pipeline started.\n",
        "\n",
        "transaction_list = []\n",
        "\n",
        "directory = \"/content/drive/MyDrive/data_engineer_assessment/raw_zone/raw_parquet_files\"\n",
        "for i in os.listdir(directory):\n",
        "  transaction_list.append(i)\n",
        "  transactions_filtered = fnmatch.filter(transaction_list, 'transaction*') # Find all files with transaction data\n",
        "  customer_filtered = fnmatch.filter(transaction_list, 'customer*') # Finds all files with customer data"
      ],
      "metadata": {
        "id": "bXyqQkTzujWi"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transaction_data = spark.read.parquet(\"/content/drive/MyDrive/data_engineer_assessment/raw_zone/raw_parquet_files/{}\".format(transactions_filtered[0])) # Reads first parquet file in transactions list \n",
        "customer_data = spark.read.parquet(\"/content/drive/MyDrive/data_engineer_assessment/raw_zone/raw_parquet_files/{}\".format(customer_filtered[0])) # Reads first parquet file in customers list\n",
        "\n",
        "for i in range(1, len(transactions_filtered)): # Loops through all available transcation files \n",
        "  transaction_data.write.mode('append').parquet(transactions_filtered[i])\n",
        "\n",
        "for i in range(1, len(customer_filtered)): # Loops through all available customer files\n",
        "  customer_data.write.mode('append').parquet(customer_filtered[i])\n",
        "\n",
        "# Creates one main parquet file with all other parquet files appended. \n",
        "transaction_data.write.format('parquet').partitionBy(\"transaction_id\").save('/content/drive/MyDrive/data_engineer_assessment/refined_zone/Refined_Parquet_Files/transaction_data_{}_{}_{}.parquet'.format(date.day, date.month, date.year)) \n",
        "\n",
        "# Creates one main parquet file with all other parquet files appended. \n",
        "customer_data.write.format('parquet').partitionBy(\"customerid\").save('/content/drive/MyDrive/data_engineer_assessment/refined_zone/Refined_Parquet_Files/customer_data_{}_{}_{}.parquet'.format(date.day, date.month, date.year))"
      ],
      "metadata": {
        "id": "zyuwFiHiunP1"
      },
      "execution_count": 50,
      "outputs": []
    }
  ]
}